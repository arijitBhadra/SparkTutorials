## Index
![deep](https://user-images.githubusercontent.com/12748752/144213667-6c093ddd-04d3-455a-9c7b-57b3cb7fa5ed.png)
![light](https://user-images.githubusercontent.com/12748752/144213674-89ceed2c-eca9-43ab-a8cd-53385fe25440.png)

### What is Apache Spark?
![deep](https://user-images.githubusercontent.com/12748752/144213667-6c093ddd-04d3-455a-9c7b-57b3cb7fa5ed.png)
* Apache Spark is a cluster computing platform designed to be fast and general purpose.
* Apache Spark is a fast, in-memory data processing engine with elegant and expressive development APIs in Scala, Java, Python and R that allow data workers to efficiently execute machine learning algorithms that require fast iterative access to datasets.
* Spark on Apache Hadoop YARN enables deep integration with Hadoop and other YARN enabled workloads in the enterprise.

### Resilient Distributed Datasets (RDDs)
![light](https://user-images.githubusercontent.com/12748752/144213674-89ceed2c-eca9-43ab-a8cd-53385fe25440.png)
> #### "In Spark all work is expressed as creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result."
> #### RDD is -
  * _**an immutable,fault-tolerant collection of objects.**_
  * _**can partitioned and distributed across multiple physical nodes of a YARN cluster.**_
  * _**can be operated in parallel.**_
  
* There are two ways to create RDDs:
    1. parallelizing an existing collection in your driver program , or
    2. sgsg
      A. kolkadfa
      
            
    
    
* b)referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.






 
